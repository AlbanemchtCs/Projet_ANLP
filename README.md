# :globe_with_meridians: Projet Intents Classification for Neural Text Generation
Projet pour le cours d'Advanced Natural Language Processing √† CentraleSup√©lec. 

Le sujet du projet peut √™tre r√©cup√©r√© sur [GitHub]{https://github.com/PierreColombo/NLP_CS/blob/main/project/project_3_intent.md}.

## üéØ Objectif
Le but du projet est d'impl√©menter un classificateur d'intention. 

## üìå Contexte et enjeux
L'identification √† la fois des actes de dialogue (DA) et des √©motions/sentiments (E/S) dans le langage parl√© est une √©tape importante pour am√©liorer les performances des mod√®les sur les t√¢ches de dialogue spontan√©. En particulier, il est essentiel d'√©viter le probl√®me de la r√©ponse g√©n√©rique, c'est-√†-dire qu'un syst√®me de dialogue automatique g√©n√®re une r√©ponse non sp√©cifique - qui peut √™tre une r√©ponse √† un tr√®s grand nombre d'√©nonc√©s de l'utilisateur. Les DAs et les √©motions sont identifi√©s gr√¢ce √† des syst√®mes d'√©tiquetage de s√©quences qui sont form√©s de mani√®re supervis√©e. Les DAs et les √©motions ont √©t√© particuli√®rement utiles pour former ChatGPT.

## :page_facing_up: √ânonc√© du probl√®me
Nous commen√ßons par d√©finir formellement le probl√®me d'√©tiquetage de s√©quences. Au niveau le plus √©lev√©, nous avons un ensemble de conversations compos√©es d'√©nonc√©s $D$, c'est-√†-dire que $D = (C_1,C_2,\dots,C_{|D|})$ avec $Y= (Y_1,Y_2,\dots,Y_{|D|})$Y est l'ensemble correspondant d'√©tiquettes (par exemple, DA, E/S). √Ä un niveau inf√©rieur, chaque conversation $C_i$ est compos√©e d'√©nonc√©s $u$, c'est-√†-dire que $C_i= (u_1,u_2,\dots,u_{|C_i|})$ avec $Y_i = (y_1, y_2, \dots, y_{|C_i|})$ √©tant la s√©quence d'√©tiquettes correspondante : chaque $u_i$ est associ√© √† une √©tiquette unique $y_i$. Au niveau le plus bas, chaque √©nonc√© $u_i$ peut √™tre vu comme une s√©quence de mots, c'est-√†-dire, $u_i = (\omega^i_1, \omega^i_2, \dots, \omega^i_{|u_i|})$.

## ü§î Choix techniques
### üìä Dataset
Nous avons repris le dataset utilis√© dans le papier original suivant [Code-switched inspired losses for spoken dialog representations]{https://aclanthology.org/2021.emnlp-main.656/}. Le dataset complet est disponible sur [Hugging Face]{https://huggingface.co/datasets/miam}.
Voici la composition des diff√©rents sous-datasets:

| Nom du dataset           | Langue                                             | Train                    | Valid                    | Test                    |
|--------------------------|----------------------------------------------------|--------------------------|--------------------------|-------------------------|
| dihana                   | Espagnol                                           | 19063                    | 2123                     |2361                     |     
| ilisten                  | Italie                                             | 1986                     | 230                      |971                      |    
| loria                    | Fran√ßais                                           | 8465                     | 942                      |1047                     |    
| maptask                  | Anglais                                            | 25382                    | 5221                     |5335           |             
| vm2                      | Allemand                                           | 25060                    | 2860                     |2855   |         

Nous choisissons le sous-dataset `loria` qui semble un bon compromis entre nombre d'√©nonc√©s et pr√©cision des r√©sultats.

### üî° Tokenizer
Nous faisons le choix d'utiliser un tokenizer mBERT (multilingual BERT) qui est une version multilingue du mod√®le BERT (Bidirectional Encoder Representations from Transformers), qui est pr√©-entra√Æn√© sur un grand corpus de textes dans plusieurs langues.

mBERT permet de traiter plusieurs langues sans avoir besoin de mod√®les de langage sp√©cifiques pour chaque langue, ce qui permet une meilleure g√©n√©ralisation.

De plus, mBERT est particuli√®rement efficace pour le traitement de textes complexes, tels que les textes scientifiques ou techniques, les textes juridiques ou les documents gouvernementaux.

### ü§ñ Mod√®le



## :card_index_dividers: Segmentation
Notre r√©pertoire est segment√© en X fichiers python, X jupyter notebooks, deux fichiers markdown, un fichier .gitinore et un fichier texte pour les requirements :

```bash 
.
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ CONTRIBUTING.md
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt 
‚îú‚îÄ‚îÄ X
‚îÇ     ‚îú‚îÄ‚îÄ X
‚îÇ     ‚îî‚îÄ‚îÄ X
‚îî‚îÄ‚îÄ X
      ‚îî‚îÄ‚îÄ  x

```

- ``README.md`` contient l'ensemble des informations sur le projet pour pouvoir l'installer.
- ``CONTRIBUTING.md`` contient l'ensemble des informations sur les normes et les pratiques de collaboration et de gestion du projet.
- ``.gitignore`` contient les fichiers qui doivent √™tre ignor√©s lors de l'ajout de fichiers au d√©p√¥t Git.
- ``requirements.txt`` contient la liste des modules et des biblioth√®ques Python qui doivent √™tre install√©s, ainsi que leur version sp√©cifique.

## :wrench: Installation
Pour lancer, nous vous recommandons sur un terminal uniquement :

1. Tout d'abord, assurez-vous que vous avez install√© une version `python` sup√©rieure √† 3.9. Nous vous conseillons un environnement conda avec la commande suivante : 
```bash
conda create --name intent_classification python=3.9
```
- Pour activer l'environnement :
```bash
conda activate intent_classification
```
- Pour acc√©der au r√©pertoire : 
```bash
cd projet_anlp
```

2. Vous devez ensuite installer tous les `requirements` en utilisant la commande suivante :
```bash
pip install -r requirements.txt
```

Ex√©cuter ensuite les notebooks jupyter dans l'ordre suivant : 

1. X
2. X
3. X
4. X

## :pencil2: Auteurs
- MICHOT Albane
- NONCLERCQ Rodolphe



